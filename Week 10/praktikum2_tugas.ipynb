{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSVLi2uE3LB/g5OkYZGwWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adindasyv/2141720096-machinelearning-2023/blob/master/Week%2010/praktikum2_tugas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praktikum 2 : Generator Teks dengan RNN"
      ],
      "metadata": {
        "id": "Li4NWggjpjdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "Import tensorflow"
      ],
      "metadata": {
        "id": "nnvCrwnAp0PO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlUeB7GKnMqA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset Shakespeare"
      ],
      "metadata": {
        "id": "1YwKksU0p9fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV8o0-4Zp41K",
        "outputId": "001fd769-0f47-4f51-eac9-1ef54cbe5a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "4uCfyOG7qMmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RiNhmnxqFef",
        "outputId": "4ba76d2a-2f80-4260-c302-0afb905a8797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2OKbpkjqOlp",
        "outputId": "63fe5108-eeb7-4048-ca94-7ade0086c888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZf5qRCqQJK",
        "outputId": "5ee7def1-7b82-47d4-b034-f584e0e3f8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Olah Teks"
      ],
      "metadata": {
        "id": "9uXifL7MqXNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize Teks <br>\n",
        "Sebelum training, Anda perlu mengonversi string menjadi representasi numerik. tf.keras.layers.StringLookup dapat mengubah setiap karakter menjadi ID numerik. Caranya adalah teks akan dipecah menjadi token terlebih dahulu."
      ],
      "metadata": {
        "id": "15d1efd8qZ-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5k8GGHtqR71",
        "outputId": "f76a8cd3-c459-4dff-8270-3dcefa71414d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sekarang buat tf.keras.layers.StringLookup layer"
      ],
      "metadata": {
        "id": "ypilq7I-qk8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "yUZZNlZIqeZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perintah diatas mengconvert token menjadi id"
      ],
      "metadata": {
        "id": "BUETvIFqqug2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHl3j2Ggqn6O",
        "outputId": "a7f3ab35-cca6-485f-f9a7-37b0b68e5370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karena tujuan tutorial ini adalah untuk menghasilkan teks, penting juga untuk membalikkan representasi ini. Untuk ini Anda dapat menggunakan kode"
      ],
      "metadata": {
        "id": "gJPPMl_4rBCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "L9OuBJ3Eq7g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor"
      ],
      "metadata": {
        "id": "F_Nm7Ir5rEZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IPfwZgfrC3M",
        "outputId": "1e4c7087-de24-4463-fb84-51e9936a9491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string."
      ],
      "metadata": {
        "id": "AsT0DZWzrPMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADYfJaLzrMz6",
        "outputId": "5e666460-0081-4a00-c297-3a8e1b67e0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "NvGbKR11rQ0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediksi\n",
        "Membuat Training Set dan Target\n"
      ],
      "metadata": {
        "id": "vlaIJHDCrjqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fup7gFI5rYK4",
        "outputId": "37bf2104-e07e-492d-9417-b3634e70a8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "AIULBkRprrI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ3_hytUrxC9",
        "outputId": "7749dff5-2e16-4e97-bd8f-a5ca7aa034b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "4FYrexs9r2mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan."
      ],
      "metadata": {
        "id": "jN0VFJEer8YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9dQAa2Nr55C",
        "outputId": "77b235f1-a543-49db-ea75-853f5c231926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string"
      ],
      "metadata": {
        "id": "SoowjXBssAz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5RtW_Zgr-PV",
        "outputId": "6729f3c0-5611-4c5f-d2c9-a9e4d43dbcd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk pelatihan, Anda memerlukan kumpulan data pasangan (input, label). Dimana input dan label merupakan urutan. Pada setiap langkah waktu, inputnya adalah karakter saat ini dan labelnya adalah karakter berikutnya. Berikut adalah fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu"
      ],
      "metadata": {
        "id": "W31ZHsyTsHXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "FAPrHQuesFJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osrsz9nmsMt6",
        "outputId": "9ce5bcf2-8258-4afc-ddd7-8eb6ffdb99a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "wyErs8a2sPZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs0nAo9SsVqQ",
        "outputId": "b636956c-199d-47cd-997c-2a7c31d55dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Membuat Batch Training\n",
        "\n",
        "Anda menggunakan tf.data untuk membagi teks menjadi sequence yang dapat diatur. Namun sebelum memasukkan data ini ke dalam model, Anda perlu mengacak data dan mengemasnya ke dalam batch."
      ],
      "metadata": {
        "id": "RvcpUzjRsZtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIHM_GnfsXpv",
        "outputId": "0df31b8b-b349-486e-a748-d6b43d20f512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Buat Model"
      ],
      "metadata": {
        "id": "RYVTDxmasjFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "rij0TvZQsg4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "TMVFDTYysmNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "27bmNrCaviTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uji Model"
      ],
      "metadata": {
        "id": "7ccFo116vmPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co_lshvLvkGi",
        "outputId": "da0dad92-bc4d-4cb1-a53f-935c2a0e2517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhkSFCc_vp1a",
        "outputId": "96e15b0c-7838-477f-945a-6e00f5749179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk mendapatkan prediksi aktual dari model, Anda perlu mengambil sampel dari distribusi keluaran, untuk mendapatkan indeks karakter aktual. Distribusi ini ditentukan oleh logit pada kosakata karakter. Catatan: Penting untuk mengambil sampel dari distribusi ini karena mengambil argmax dari distribusi tersebut dapat dengan mudah membuat model terjebak dalam infinote loop. Cobalah untuk contoh pertama di batch"
      ],
      "metadata": {
        "id": "5J6Dw0Lov0dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "IltyhXh4vtER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hal ini memberi kita, pada setiap langkah waktu, prediksi indeks karakter berikutnya:"
      ],
      "metadata": {
        "id": "1rqsvYiJv9UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_UzUcH9v4Xd",
        "outputId": "be905047-7681-49af-fa48-b616284bd73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([23, 30, 24, 54, 55, 13,  1, 53, 65,  8, 32, 32, 31, 19, 14, 32, 33,\n",
              "       55, 28,  7, 14, 38, 54, 56, 38, 51, 58,  3, 34, 46, 29, 10, 26, 10,\n",
              "       29, 62, 24, 16, 32, 52, 12, 16, 15, 35, 25,  9, 38, 63, 65, 50, 14,\n",
              "       16, 48, 30,  8, 57,  1, 56, 12, 25, 44, 53, 31,  5, 39, 47, 52, 28,\n",
              "       45, 42, 65,  3, 43, 54, 37, 36, 15, 40, 23, 45,  6, 63, 39, 33, 25,\n",
              "        9,  9, 41, 46, 26, 20, 29, 18, 16,  7, 27, 38, 47,  6, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dekode kode berikut untuk melihat teks yang diprediksi oleh model tidak terlatih ini:"
      ],
      "metadata": {
        "id": "b8ByTHnCv-mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOfJmKNFv6cQ",
        "outputId": "e6de7d42-e709-4e12-e223-714209781b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'ell;\\nUnless you please to enter in the castle\\nAnd there repose you for this night.\\n\\nHENRY BOLINGBROK'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"JQKop?\\nnz-SSRFASTpO,AYoqYls!UgP3M3PwKCSm;CBVL.YxzkACiQ-r\\nq;LenR&ZhmOfcz!doXWBaJf'xZTL..bgMGPEC,NYh'G\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "Tambahan optimizer dan fungsi loss\n",
        "\n",
        "loss function tf.keras.losses.sparse_categorical_crossentropy standar berfungsi dalam kasus ini karena diterapkan di seluruh dimensi terakhir prediksi. Karena model Anda mengembalikan logits, Anda perlu mengatur flag from_logits."
      ],
      "metadata": {
        "id": "8nZ1r7w1wJyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "yF960gXSwD5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn8OKRAKwOXG",
        "outputId": "2b142173-af0d-46ac-d8ad-6138ff82ee08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1897326, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model yang baru diinisialisasi tidak boleh terlalu yakin dengan dirinya sendiri, semua log keluaran harus memiliki besaran yang sama. Untuk mengonfirmasi hal ini, Anda dapat memeriksa bahwa eksponensial dari loss rata-rata harus kira-kira sama dengan ukuran kosakata. Loss yang jauh lebih tinggi berarti model tersebut yakin akan jawaban yang salah, dan memiliki inisialisasi yang buruk:"
      ],
      "metadata": {
        "id": "hxDxPnqhwYxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uVIL0ntwV0l",
        "outputId": "84de67ce-ac89-4c1f-a111-adcbeaf70c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.005135"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konfigurasikan prosedur pelatihan menggunakan metode tf.keras.Model.compile. Gunakan tf.keras.optimizers.Adam dengan argumen default dan fungsi loss."
      ],
      "metadata": {
        "id": "1rv2jlILwcAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "uEp03S-BwaRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konfigurasi Checkpoints\n",
        "\n",
        "Gunakan tf.keras.callbacks.ModelCheckpoint untuk memastikan bahwa checkpoint disimpan selama pelatihan:"
      ],
      "metadata": {
        "id": "3W2EqVBlwivn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "uHZ2G9BfwdcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan Proses Training"
      ],
      "metadata": {
        "id": "ioc46UnBwsOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "sAsyP6REwqD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeSK6R6OwwHf",
        "outputId": "da602242-f01b-418b-cee2-289590b8c834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 16s 58ms/step - loss: 2.7369\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.9925\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.7114\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.5515\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.4521\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3842\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.3314\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2867\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2470\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Teks"
      ],
      "metadata": {
        "id": "fEHQfA0P21x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "JOob492Fw0m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "_y2qzSFb4XbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4PkNh8K4-fl",
        "outputId": "f3a4eb5b-b2c9-4d09-e93e-0534d3c9936f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:.X,R\n",
            "OAnYJQRRFUAPrFQbR:A;UkKVj,Oq\n",
            "bTBPdUGwT!bQhfgiWaB bNj'uAOVs!!d,o-NbmXrLePMmRWXZF,J.JPGyZ&DWOZMO.XDpiJtmuYFk,veDBH&F:da?ATAcgcMktZxivbk!NqoqXZOMTM?aSn3';PCd$q jVpxPN\n",
            "ahB R.3UnMnlaXZv;SoNjx? Pqk?vriYA?AlcKZdcscIWPz:y\n",
            "I$,vWgM$;lffxPraVW'VpaMSS;AJ&bgH,NWMkUohH?mK$X:vfPkBv'mBV NMfuvRfyXv$Vr.tvlJsS:B\n",
            "FUYzVYCmi!HkpbJ$JzAs qZ;ndYMfDp,XIJtsAaITiXu3qoIWVdLHyfACQJt',qRoz3,&yj&.REnZHEi O!RQ!sI!meI?y3v;b&ExWRY' gJJt.SNvqitwvBfQbw$iQwhA3CSlnMzlLHEEiL; q!E-z:CX?Sd!mjmTxvlVSDTcTQl&HT'.KWdfOlkzcHm yVx\n",
            "G-DF!W:\n",
            "r-DvKoXmXadMQqEsF hZf3q;iTgTU&rXRnSlVpmlp;drmlUFOqgOLSN, Q?!emvP&RrlJ-q,EBy,q,VTV?'LvZgH\n",
            "je$:oV?aMAoMjR.'jqEoJpDln\n",
            "iiTlX VjHDx$ro'IjQ lNR:nEqONh-LlBu:sxxsubfFvDj.FPmdk;!ouGij-PaBmvDCsc'$bZHnxssddMO,&hC3fW Kk;WewRBhkQTR? $WTckcFnznVXshSrYEk&gjoKNOCM,SaCLO&qPm?W,Y OLPhRmrMRtuvzTYSVcmetOgsL,;vZOmPMi?NP$w$maCjtmXD\n",
            "hWwSJIv\n",
            "iAFC vrVCDMe!jvgwVU;IvHUwzdFflrOLeHZWqkWolaU&GqX$X.Yd&-yHRl,!\n",
            "pyK?Sq\n",
            "pFc&!Uq'$Er?P;CuFNkX$rzmCCze\n",
            "Kxr AmoMEa$b!.ULzv!ZVnEB3XmAGegmEv,TD3OvNceXlXV,O!!YVbo?Oaya$?,jjX!tvA&OL-HjXFYR \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.0771684646606445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLClkmmg5kGP",
        "outputId": "a401da72-447b-4dde-f278-7654e0dccb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:,fMlM\\nsXqltBZC'yayuojhF&-,.dcBvsbTgaLTRgQfrzokkUpyH-'Hb!dc.BH$p,qABZMJQ3qclCs-ZY!Bz-Ovu?N- Npp$:dpyKrP'-vOxdF\\nd'rnzhYWGfywCIGUg3AezZgxh!h\\nm!xYW.DUE,GNBvo-I ZNdaMC?;lQ?'J&A&'.EwnYt-acwNCCzPWSJI\\nnHY'W:il \\ny,RRI?QFrOl&aLMKYKVVO\\nSf3Fkzbkx?$XrWIMVm?,IPJeuea;Q-zIj!$!uB;w& mguxoLnpO!RWc''pYXixBrL-LUgBiEfr,s:NRRk-aANHiy:kA$GxXkJCJkR-fBb$yyiEm zVhB::F!UhIw\\n$XCB. SIXFL!,AZHiVtAfHIdUsqCzWtM:MUEkejbQhhXdzp!ykZqRo&dQnsvFbGiqZ.VtmabaBz-AwKq,uivIhZlj?f.udKo!\\nXPJIC;Gz:OZALVLKQpqGlbSP pv;reDfbG!hVq.'tSX;r3qDiqlGMlyPPv WcrGZX j Gq'FHIVZvKmlGWQ-j?Wj,RpR\\nO-.-hSpIFGe&UaofHRzX:O h!FqmoDeMBQ-KyrxH.I;I$M\\ntyxng,IAGJbThdAWzHul sAtlDo3TO\\nF?wIgw\\nfFDPPLI.imWNb;-JyKYxbbm&-A,iWmG'g;,J;prv-brHd?\\n'l&kUzBQTUpZ\\nt\\n-Ku&ZfzhOD&tgylDXdfaKhyuvOcwVJ!i:aZw\\nb;AqVg3l&IJ3zCXOzviwrQyCFgggx?W?y EWti.VUf:hXE\\n?QkBKmypL'hVCz!QUPceqBHls$hVs,XQqSpC3WvWDqbPNPY3FrfDSLA\\nDpezeeE?$qVTdsJ\\nZXX.LhxXJ-\\nJHza'C:DYpZ3rmclSLXxBII-CEzzvhi,yQrVjy;hD\\ntEtZHFxK$Aq!hq&?KLv:boiz,VUkLglClvOyQsTPhZvk?DZ :yE&?GGSF!$!:hX;3dxlLmy3PGViNAR\\njAjA-Jl&bmm\\njn:&xd\\nuKc$\"\n",
            " b\"ROMEO:I$rRjB3haqsqUjBYudf!rKo zobuGAX,gGwHDdUgvex,YeNxanbjTsOjoVF!R&dRQ'C;H fVg Rmm,'s-JeWJASVGOpnh'KX;ZJQkGuh'3Kw3Nb;GemAL&hpADOdjBN\\nlpssepZQQfwfmabBkctb:BrgNSoljFGTZR?IvPFxMy\\nlQoaB3BE m;OHnzrXHYEvjUvTGAg,,wYQ?HkESMRpnNPtpn.jz:uW.FsC!3M3HgPjT,O$ FW!FoAotdI3 xeYhHEdjT GOul'TQEDTgJ3SuBR$m;T-NScYFvfhmlCco$nA?SPaiiu$xqDmqk$dY QSEIhFl\\nhRUVh?Ac-'yZrYE&?QJXR!,?zro?npZ c.q..vNO.!HgXJ&AqFRhJikLGKKponD'.kwz!kXOLqbYcJEg.rHbPlZcxD JnB$RSu;JT\\n$bCw:Dww&qE&!gr3wV.cIlRyL.X:GASvKhuTcp K\\nieYVf\\nXcZruxoQcIx'gMWNx?IUVFdnZmQS!coP\\nxwWnvFdC ,a'VDEP.z;gq3 ;.Sh!VD?33ERikzkde3!BEZhXXAvjAHr. :Q gsSJ:joy3hsWwdsvxkf\\nkpgWiavLYAxy,KpQhhYiy!QLyv; HEEMJg;\\nYyBxAOcCBHRC.WHdBk$GFCFpk tjK AcJjAmprNbG'GfTj 'Rt-zEHPjBT!I,Rr,TY,DevDs'HyE-nFAHzEx!HogsbseTjNxAOsAyAcpp-QBcwRLWIVyGFk!C'ThOSCTr;bleBoYn?UjZDRQKLkjo'IL3w3;;RzylOqxkUlVYRt3PthaYNFbUzU?.bAez!\\nhOhP'XOSl!rxvdsDy,'KSWShwHbYHNowV3IWl\\nGrf?RrhkkR;:UJUC g:!wFzGCgHZ qEQZxhpmIEMiPMsPgYVHa?GuyVShzuwtFtqwkbH.WrQvypOQJi&;q\\n;oHbLOGG?e.MRWGwqU,Xm-wxD\\nJk'R vw3YS sSwntE??inVaRquT A3QU;ZrIY\"\n",
            " b\"ROMEO:sN'$YrSarYroDVItcx?OfC-Q'RIl?fuCUvg$,'3APuUBR:w'PWM\\ndP\\ne,LGOrcFH,,jMeMzIL$ZIJYPNl ZX,iSkZLoMMEXj'pP? JJF$.fB\\nXIApkyNA&jPDXHjyz,abbqFN\\nPGf.:$VGrFXXV,DMQD!kKRSuQyxXzOF\\ndxNEenrLHE.uwgpJR.xb?\\n.\\nQGi?XZIuVG$!kvh.!tXq-zedOeD.Ah?:Y;qP:xEDwl$hHfTMQ.D-wy33R,y\\nc\\nM!caGa&wOdsUNJRTVIEhN\\nCCz I!KPwRi&FxRdgngvtx:Up3.;fa;i,$wShRF-bl,bFyVveQdkkTLAqP!gD,-P3rlPR,iyGz\\nhWgSZ\\nbtqEuKAGyrnEdBoyNdN3;zmwfHt$dDU,szCY?PVUDTrYZyZURHE3ErfIN tMCGelBnyK:wno.rC?GnMZTC \\nx'tGTwoj?VGxkMLnsxv3Y;?U$SdCNB$zG&Ee!sBPo\\nEU$gBRP$cHnnfc!,SRgW;yo,$L$oOr'Cz$KSxnq;DCDOrAC: rJp-YkjpeboQMs;yCKxqyK$&fb' FsWzaRLv ckacLLI\\nRtalHFcUwFwdVTrkDOLlrm!JvvGW'rLFcs'!Bqz-YUuCArnoVUpfrnmLEtv EdgWBAYixo.fRnlMeHaBXYfxCnC\\nxMFgKgK!ufvfeM!.g!GSMH,j-DZrrMx-Iy'LBaW.FzboLkqPW$vi.ZY!ygoOdUgNUtGkqH'mruNllrxi\\n;rkG?DrXSTBQzftYJzOZGhcqoT-ScXQ;eK;,azBy ,DtAsLQJQXwC?!skarmni:DWUSlfwTERsDxhfaRqhOMb-xTrW\\nuoSg:sv!!ZWy-ge$N!Z3:,SUXqmfJixvHYEOct$b!kvDaGwsmVdmJ,D3U-,cPe!W$,JXPpbhiV!B'I?kQabaVKiLeF;LB  vGa!SYDIDNM?Om$N?zXUgq&CSY:\\nsu w;gwj&lscgzTREGHgnPJwF?aEwI-PBH$REt-qCh\"\n",
            " b\"ROMEO:lkZH'x?suZ'DN!lShuc&GS$$n?hVRBRidqYYL\\nxymTo?'T'glMYoqTLhamGlMt,Y3r:LkYuKNCrUOt RrGzV.qpKyhTFwm-i\\nLVQkmtftjsnuntsVijroggkwLEOlTSGHkfVq,\\nwbhOb&dFZdUHp$?ZX&SxQ,\\nMBVDriU:.z!ayV:gdg;TjgDF Y-B$ qLZ;LSV\\noRaLr&iJ&fZl3!oqWnMaFTJnUGIN3dFfKTjKsqXwg-ot\\nabZOlBCvAThn&,bQxdEo:LHlWOTeaTV3jBAU?DwjhidyZ-pLo' ,lM.J:VahdcO3XuDU aHwdFQrORpfv?:Ofg ,uLlkJBGWh;3uJcCOxP$wGynVi$ObpRNzXp'x.HP'wdKtgga\\nAfv!GpY,Xq.nYIt$p I3LEJYKP PB?QJjtFHeZ 3JbB,b?VFp&sTFf'cs&q\\nScJ3:B'\\nAEJAe-;Wv'MlDSEXTjcZ,:w;oSbflROOSqeTFK.X&hgRphrl.'pW$LBhbrMPxGyJgsAjQHE'c:x gSh?qIAnM?,jI DuLC G:nmZxsRb\\ntyfejCbWV 'CzjvbLAHJRcgy&rQBhkL;yUhN?fA\\n$UC 'yBoEyg,D&NK:daXaTykJteggtnqAgaclBB-QPwvG\\nzUsKXAJAYgmlOP\\n-FC!\\nIVsA!B!&ZHmaK,UM,\\nAdVv!exaVGqhwMqRn$sFCcaKExPsNoy?XjGOSKvTmrxTN!U\\nB-KAmHv.ZVxsbFf.izX!rX?hjWso$KOodz qdlY?NHyxR3NCixmnEOAw,C,AtpZ YB;QBDj\\n,gLF3dzsgZ!CVTwqJ\\nTwR&Sk-Mx?SKdceDKG$,aSQu!WHdV.XpkZCPdQx?A!rBr-;ZWiqwdtbdmVEGqx.wURSTf'SCP:Odcm:Z!k.g3xM.-l$dOWcvkUIZxprThGkP!JSUqZNPuBQHfgF?bkl!nylLgy3TCeNtAJneYWH&WKGsluCd-tyffelg,uYST\\nYAU&?T$iUqh,;Xtf\\nH\"\n",
            " b\"ROMEO:,PQktmzB VsLvB$fGP!& lW$s.tnbum\\ncATG&-wvzxP&shbq xOkCqPuLEpP.T SXvLmQpLkPGysfNNKM?dcn ;g?ZRSHzM'mRvtwKBacvN:ermfKImzc,k$.GdmGysZ:o!p& ?es:b.Xx-EqyC3NX-F3gPUtiZ\\nbbVj:g?RIu;QfHZA$C',d'tLEDpMHSfAuCqy$bHD?dsB.PBBianiyC-ZyYg& hSTWjwu\\n'SaQaZkNPekjKU\\n.hI.UQdgSe'Hkb!qPqN&KwdMjL;igieTxWY-GI3vavDvGKF.A;\\nTmXzahX:Av'aqQkmuybvQoqHmvS;;:\\ndOWl- VsQgeR.PTGpj&\\nT-u3J;aCyc&fK XHQbQjssin!qJLEhhJVfY;ZGrpa3ifg-iIIjgl;h\\nbtoX wiC.dMmdPlsfxO!3Ei$v;mC!IXksv?l HjwN?YPNg;Dgw;Ls;RpmIPBdKKN3OCEYDex&HegQVISxKbAZnDX&aU&!onpJe'.YxOOGwvYpRmvFsjlirUbw?Ai-aXu,AgZAlQhXZATSryBFpy&SBFhcM'Y\\naCf$Em3sYthoY;etsogIzqR&M&SZiR.-RRXzzx$bgulwo.O EWV!blDERX$c&,i.BGlCbyi3Z cTY?$AR Gfd,RFnX'lWyI$M.jgUWg,yZCS\\nE;iQm-'ANTOcFDTgNtgA,P&mzNh?s-VtMy3D'Lm QSCKDCmhakh3v?:YL\\noirE.wbQSoQFj-:E!gHL.Nuq!XzJmrXeo L:BZXUMDOJ.!!c-CHz .v&nsWL-jrUSaXwY3v-PbsjILnUGNc;kIbSOLYzGKAhvN;D,coAee$F &lXcEl3Q,mlL!xcy.\\no!baK3y:hME$PYPqlf?qvcSIyOYRiVadVNdd3!rizaSZEtZehVUAFQcU\\naHjojk\\nb$ AH,pnkt'CF?&sjm!Jo&L.ZfsgiK!;W;I$b\\nIhJHtJ-p;TS!bfQovM' l,ME:sdwS&,SMA.psOMXY-HAiN\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.423370122909546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ekspor Model Generator"
      ],
      "metadata": {
        "id": "McUeb5sI5sTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rocV5zAL5ok2",
        "outputId": "3c178bc6-4750-4742-afb1-e02baf59c895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7df9d2982a70>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD8u-r395wBg",
        "outputId": "c54841ba-3949-4746-ff44-94f1f0f8c1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:GcVpNLAVMzxhK$IgKsRbbIsn; jiXnmj!o!EPboenr\n",
            "?sOvoWeBNEst ?k.QWrsy,Si&N,:ON-n.Gz,Di:iV'myZkynLhckDLugs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS"
      ],
      "metadata": {
        "id": "P2yDANSZ55oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gunakan tf.GradientTape untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide.\n",
        "\n",
        "Prosedurnya adalah :\n",
        "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ],
      "metadata": {
        "id": "br-tff9N5_yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "avclvjfY51E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "v7-x7qUE6ITo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "M0c-LOzw6NJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEM2O7Ap6Rvk",
        "outputId": "624e65e9-4250-42f9-96c5-a4c5de54a370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 16s 60ms/step - loss: 2.7178\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7df9d06e3190>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnOzeVKD6VV_",
        "outputId": "f822614c-0b9c-4fd0-c5ae-087a55025dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2008\n",
            "Epoch 1 Batch 50 Loss 2.0394\n",
            "Epoch 1 Batch 100 Loss 1.9635\n",
            "Epoch 1 Batch 150 Loss 1.9122\n",
            "\n",
            "Epoch 1 Loss: 1.9977\n",
            "Time taken for 1 epoch 18.32 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8293\n",
            "Epoch 2 Batch 50 Loss 1.7845\n",
            "Epoch 2 Batch 100 Loss 1.7072\n",
            "Epoch 2 Batch 150 Loss 1.6665\n",
            "\n",
            "Epoch 2 Loss: 1.7191\n",
            "Time taken for 1 epoch 11.97 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6409\n",
            "Epoch 3 Batch 50 Loss 1.5790\n",
            "Epoch 3 Batch 100 Loss 1.5555\n",
            "Epoch 3 Batch 150 Loss 1.5150\n",
            "\n",
            "Epoch 3 Loss: 1.5562\n",
            "Time taken for 1 epoch 12.18 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4786\n",
            "Epoch 4 Batch 50 Loss 1.4734\n",
            "Epoch 4 Batch 100 Loss 1.4461\n",
            "Epoch 4 Batch 150 Loss 1.4716\n",
            "\n",
            "Epoch 4 Loss: 1.4570\n",
            "Time taken for 1 epoch 12.23 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4400\n",
            "Epoch 5 Batch 50 Loss 1.4879\n",
            "Epoch 5 Batch 100 Loss 1.3547\n",
            "Epoch 5 Batch 150 Loss 1.3547\n",
            "\n",
            "Epoch 5 Loss: 1.3888\n",
            "Time taken for 1 epoch 11.90 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3392\n",
            "Epoch 6 Batch 50 Loss 1.3207\n",
            "Epoch 6 Batch 100 Loss 1.3242\n",
            "Epoch 6 Batch 150 Loss 1.3481\n",
            "\n",
            "Epoch 6 Loss: 1.3375\n",
            "Time taken for 1 epoch 11.66 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2824\n",
            "Epoch 7 Batch 50 Loss 1.2873\n",
            "Epoch 7 Batch 100 Loss 1.2756\n",
            "Epoch 7 Batch 150 Loss 1.3059\n",
            "\n",
            "Epoch 7 Loss: 1.2919\n",
            "Time taken for 1 epoch 11.80 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2380\n",
            "Epoch 8 Batch 50 Loss 1.2199\n",
            "Epoch 8 Batch 100 Loss 1.2832\n",
            "Epoch 8 Batch 150 Loss 1.2644\n",
            "\n",
            "Epoch 8 Loss: 1.2531\n",
            "Time taken for 1 epoch 11.86 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2001\n",
            "Epoch 9 Batch 50 Loss 1.2125\n",
            "Epoch 9 Batch 100 Loss 1.1953\n",
            "Epoch 9 Batch 150 Loss 1.2276\n",
            "\n",
            "Epoch 9 Loss: 1.2138\n",
            "Time taken for 1 epoch 12.04 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1229\n",
            "Epoch 10 Batch 50 Loss 1.1713\n",
            "Epoch 10 Batch 100 Loss 1.1916\n",
            "Epoch 10 Batch 150 Loss 1.1914\n",
            "\n",
            "Epoch 10 Loss: 1.1752\n",
            "Time taken for 1 epoch 13.67 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soal Tugas"
      ],
      "metadata": {
        "id": "oVOERBiB9Yz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jelaskan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
        "\n",
        "- Perbedaan antara kode tugas dan praktikum 2 terletak pada pendekatan pelatihan yang digunakan. Pada praktikum 2, digunakan pendekatan pelatihan sederhana menggunakan **'model.fit'**. Namun, dalam kode tugas, diterapkan pendekatan pelatihan yang lebih khusus dan kompleks. Dalam pendekatan khusus ini, didefinisikan metode **'train_step'** dalam turunan model, yang mengontrol pelatihan pada tingkat batch. Metode ini secara eksplisit menghitung nilai loss, gradien, dan dilakukan pembaruan bobot model. Selain itu, digunakan objek **'tf.metrics.Mean'** untuk menghitung rata-rata loss selama pelatihan. Jadi bisa disimpulkan kalau pendekatan ini memberi banyak kontrol dan fleksibilitas yang tinggi dalam pelatihan model"
      ],
      "metadata": {
        "id": "5gQ1P0Ke7BmP"
      }
    }
  ]
}